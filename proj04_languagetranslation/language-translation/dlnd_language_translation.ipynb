{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 语言翻译\n",
    "\n",
    "在此项目中，你将了解神经网络机器翻译这一领域。你将用由英语和法语语句组成的数据集，训练一个序列到序列模型（sequence to sequence model），该模型能够将新的英语句子翻译成法语。\n",
    "\n",
    "## 获取数据\n",
    "\n",
    "因为将整个英语语言内容翻译成法语需要大量训练时间，所以我们提供了一小部分的英语语料库。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据\n",
    "\n",
    "研究 view_sentence_range，查看并熟悉该数据的不同部分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 227\n",
      "137861\n",
      "Number of sentences: 137861\n",
      "Average number of words in a sentence: 13.225277634719028\n",
      "\n",
      "English sentences 0 to 10:\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "california is usually quiet during march , and it is usually hot in june .\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "your least liked fruit is the grape , but my least liked is the apple .\n",
      "his favorite fruit is the orange , but my favorite is the grape .\n",
      "paris is relaxing during december , but it is usually chilly in july .\n",
      "new jersey is busy during spring , and it is never hot in march .\n",
      "our least liked fruit is the lemon , but my least liked is the grape .\n",
      "the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "\n",
      "French sentences 0 to 10:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
      "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
      "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
      "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
      "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in source_text.split()})))\n",
    "\n",
    "sentences = source_text.split('\\n')\n",
    "print(len(sentences))\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "print()\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现预处理函数\n",
    "\n",
    "### 文本到单词 id\n",
    "\n",
    "和之前的 RNN 一样，你必须首先将文本转换为数字，这样计算机才能读懂。在函数 `text_to_ids()` 中，你需要将单词中的 `source_text` 和 `target_text` 转为 id。但是，你需要在 `target_text` 中每个句子的末尾，添加 `<EOS>` 单词 id。这样可以帮助神经网络预测句子应该在什么地方结束。\n",
    "\n",
    "\n",
    "你可以通过以下代码获取  `<EOS> ` 单词ID：\n",
    "\n",
    "```python\n",
    "target_vocab_to_int['<EOS>']\n",
    "```\n",
    "\n",
    "你可以使用 `source_vocab_to_int` 和 `target_vocab_to_int` 获得其他单词 id。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn', ',', 'and', 'it', 'is', 'snowy', 'in', 'april', '.']\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    def get_ids(text, vocab_to_int, addeos):\n",
    "        sentences = text.split('\\n')\n",
    "\n",
    "        sentences_id = []\n",
    "        for sentence in sentences:\n",
    "            sentence_id = []\n",
    "            for word in sentence.split():\n",
    "                sentence_id.append(vocab_to_int[word])\n",
    "            if addeos:\n",
    "                sentence_id.append(vocab_to_int['<EOS>'])\n",
    "            sentences_id.append(sentence_id)\n",
    "        return sentences_id\n",
    "    print(source_text.split('\\n')[0].split())\n",
    "    source_text_ids = get_ids(source_text, source_vocab_to_int, False)\n",
    "    target_text_ids = get_ids(target_text, target_vocab_to_int, True)\n",
    "    return source_text_ids, target_text_ids\n",
    "\n",
    "    source_text = source_text.split()\n",
    "    target_text = target_text.split()\n",
    "    source_ids, target_ids = [], []\n",
    "    for word in source_text:\n",
    "        source_ids.append(source_vocab_to_int[word])\n",
    "    for word in target_text:\n",
    "        target_ids.append(target_vocab_to_int[word])\n",
    "    return source_ids, target_ids\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_text_to_ids(text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理所有数据并保存\n",
    "\n",
    "运行以下代码单元，预处理所有数据，并保存到文件中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn', ',', 'and', 'it', 'is', 'snowy', 'in', 'april', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "helper.preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "这是你的第一个检查点。如果你什么时候决定再回到该记事本，或需要重新启动该记事本，可以从这里继续。预处理的数据已保存到磁盘上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137861\n"
     ]
    }
   ],
   "source": [
    "print(len(source_int_text))\n",
    "# print(source_int_text[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检查 TensorFlow 版本，确认可访问 GPU\n",
    "\n",
    "这一检查步骤，可以确保你使用的是正确版本的 TensorFlow，并且能够访问 GPU。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifei/miniconda3/lib/python3.6/site-packages/ipykernel/__main__.py:14: UserWarning: No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) in [LooseVersion('1.0.0'), LooseVersion('1.0.1')], 'This project requires TensorFlow version 1.0  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络\n",
    "\n",
    "你将通过实现以下函数，构建出要构建一个序列到序列模型所需的组件：\n",
    "\n",
    "- `model_inputs`\n",
    "- `process_decoding_input`\n",
    "- `encoding_layer`\n",
    "- `decoding_layer_train`\n",
    "- `decoding_layer_infer`\n",
    "- `decoding_layer`\n",
    "- `seq2seq_model`\n",
    "\n",
    "### 输入\n",
    "\n",
    "实现 `model_inputs()` 函数，为神经网络创建 TF 占位符。该函数应该创建以下占位符：\n",
    "\n",
    "- 名为 “input” 的输入文本占位符，并使用 TF Placeholder 名称参数（等级（Rank）为 2）。\n",
    "- 目标占位符（等级为 2）。\n",
    "- 学习速率占位符（等级为 0）。\n",
    "- 名为 “keep_prob” 的保留率占位符，并使用 TF Placeholder 名称参数（等级为 0）。\n",
    "\n",
    "在以下元祖（tuple）中返回占位符：（输入、目标、学习速率、保留率）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_ = tf.placeholder(tf.int32, shape=[None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, shape = [None, None], name = 'targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return input_, targets, learning_rate, keep_prob\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_model_inputs(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理解码输入\n",
    "\n",
    "使用 TensorFlow 实现 `process_decoding_input`，以便删掉 `target_data` 中每个批次的最后一个单词 ID，并将 GO ID 放到每个批次的开头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "Tensor(\"Fill:0\", shape=(2, 1), dtype=int32)\n",
      "Tensor(\"concat:0\", shape=(2, 3), dtype=int32)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def process_decoding_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for dencoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "#     print(target_data.shape)\n",
    "    print(target_data.shape)\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "#     tensor_go = tf.constant(go_id, shape = (batch_size, 1))\n",
    "    go_tensor = tf.fill([batch_size, 1], go_id)\n",
    "    print(go_tensor)\n",
    "    c = tf.strided_slice(target_data, [0,0],[batch_size, -1], [1,1])\n",
    "    c = tf.concat((go_tensor, c),1)\n",
    "#     batch, seq_length = c.shape\n",
    "#     print(batch, seq_length)\n",
    "\n",
    "    print(c)\n",
    "    return c\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_process_decoding_input(process_decoding_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码\n",
    "\n",
    "实现 `encoding_layer()`，以使用 [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) 创建编码器 RNN 层级。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: RNN state\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     lstm_layer = 1\n",
    "#     print(rnn_inputs.shape)\n",
    "#     print(num_layers)\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop]*num_layers)\n",
    "    initial_state = cell.zero_state(1, tf.float32) # batch_size is 1?\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, dtype = tf.float32)\n",
    "    return final_state\n",
    "    # my question is: in my tensorflow version 1.1.0, dtype = tf.float32 is not working\n",
    "    # maybe I need to try version 1.0     \n",
    "#     final_state = tf.identity(final_state, name = 'final_state')\n",
    "#     return final_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_encoding_layer(encoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码 - 训练\n",
    "\n",
    "使用 [`tf.contrib.seq2seq.simple_decoder_fn_train()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_train) 和 [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder) 创建训练分对数（training logits）。将 `output_fn` 应用到 [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder) 输出上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    :param encoder_state: Encoder State\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param decoding_scope: TenorFlow Variable Scope for decoding\n",
    "    :param output_fn: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Train Logits\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dynamic_fn_train = tf.contrib.seq2seq.simple_decoder_fn_train(encoder_state)\n",
    "    output, final_state, final_state_context = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "                                           cell = dec_cell, \n",
    "                                           inputs = dec_embed_input, \n",
    "                                           decoder_fn = dynamic_fn_train, \n",
    "                                           sequence_length = sequence_length,\n",
    "                                           scope = decoding_scope)\n",
    "    training_logits =  output_fn(output)\n",
    "    return tf.nn.dropout(training_logits, keep_prob = keep_prob)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer_train(decoding_layer_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码 - 推论\n",
    "\n",
    "使用 [`tf.contrib.seq2seq.simple_decoder_fn_inference()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_inference) 和 [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder) 创建推论分对数（inference logits）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    :param encoder_state: Encoder state\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param start_of_sequence_id: GO ID\n",
    "    :param end_of_sequence_id: EOS Id\n",
    "    :param maximum_length: The maximum allowed time steps to decode\n",
    "    :param vocab_size: Size of vocabulary\n",
    "    :param decoding_scope: TensorFlow Variable Scope for decoding\n",
    "    :param output_fn: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Inference Logits\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dynamic_fn_inference = tf.contrib.seq2seq.simple_decoder_fn_inference(output_fn = output_fn,\n",
    "                                                    encoder_state = encoder_state,\n",
    "                                                    embeddings = dec_embeddings,\n",
    "                                                    start_of_sequence_id = start_of_sequence_id,\n",
    "                                                    end_of_sequence_id = end_of_sequence_id,\n",
    "                                                    maximum_length=maximum_length,\n",
    "                                                    num_decoder_symbols = vocab_size,\n",
    "                                                    dtype = tf.int32,\n",
    "                                                    name = None)\n",
    "    output, final_state, final_state_context = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "                                           cell = dec_cell, \n",
    "#                                            inputs = dec_embeddings, \n",
    "                                           decoder_fn = dynamic_fn_inference, \n",
    "#                                            sequence_length = sequence_length,\n",
    "                                           scope = decoding_scope)\n",
    "#     output = output_fn(output)\n",
    "    inference_logits = tf.nn.dropout(output, keep_prob = keep_prob)\n",
    "    return inference_logits\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer_infer(decoding_layer_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建解码层级\n",
    "\n",
    "实现 `decoding_layer()` 以创建解码器 RNN 层级。\n",
    "\n",
    "- 使用 `rnn_size` 和 `num_layers` 创建解码 RNN 单元。\n",
    "- 使用 [`lambda`](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) 创建输出函数，将输入，也就是分对数转换为类分对数（class logits）。\n",
    "- 使用 `decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob)` 函数获取训练分对数。\n",
    "- 使用 `decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob)` 函数获取推论分对数。\n",
    "\n",
    "注意：你将需要使用 [tf.variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) 在训练和推论分对数间分享变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, target_vocab_to_int, keep_prob):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param encoder_state: The encoded state\n",
    "    :param vocab_size: Size of vocabulary\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Tuple of (Training Logits, Inference Logits)\n",
    "    \"\"\"\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "#     drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = keep_prob)\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm]*num_layers)\n",
    "\n",
    "    \n",
    "#     initial_state = decoder_cell.zero_state(1, tf.float32)\n",
    "#     outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, dtype = tf.float32)\n",
    "    with tf.variable_scope('decoder') as decoding_scope:\n",
    "        output_fn = lambda input_ : tf.contrib.layers.fully_connected(\n",
    "            input_, vocab_size, activation_fn = None, scope=decoding_scope)\n",
    "        train_logits = decoding_layer_train(encoder_state,\n",
    "                                            decoder_cell, \n",
    "                                            dec_embed_input, \n",
    "                                            sequence_length, \n",
    "                                            decoding_scope, \n",
    "                                            output_fn, \n",
    "                                            keep_prob)\n",
    "        \n",
    "#         def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "#                          maximum_length, vocab_size, decoding_scope, output_fn, keep_prob):\n",
    "            \n",
    "    with tf.variable_scope('decoder',reuse = True) as decoding_scope:\n",
    "        infer_logits = decoding_layer_infer(encoder_state, \n",
    "                             decoder_cell, \n",
    "                             dec_embeddings, \n",
    "                             target_vocab_to_int['<GO>'],\n",
    "                             target_vocab_to_int['<EOS>'], \n",
    "                             sequence_length,\n",
    "                                            \n",
    "                             vocab_size, \n",
    "                             decoding_scope, \n",
    "                             output_fn = output_fn, \n",
    "                             keep_prob= keep_prob)\n",
    "        \n",
    "    return train_logits, infer_logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer(decoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建神经网络\n",
    "\n",
    "应用你在上方实现的函数，以：\n",
    "\n",
    "- 向编码器的输入数据应用嵌入。\n",
    "- 使用 `encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob)` 编码输入。\n",
    "- 使用 `process_decoding_input(target_data, target_vocab_to_int, batch_size)` 函数处理目标数据。\n",
    "- 向解码器的目标数据应用嵌入。\n",
    "- 使用 `decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob)` 解码编码的输入数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "(64, 22)\n",
      "Tensor(\"Fill:0\", shape=(64, 1), dtype=int32)\n",
      "Tensor(\"concat:0\", shape=(64, 22), dtype=int32)\n",
      "33\n",
      "44\n",
      "45\n",
      "46\n",
      "55\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, \n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :param input_data: Input placeholder\n",
    "    :param target_data: Target placeholder\n",
    "    :param keep_prob: Dropout keep probability placeholder\n",
    "    :param batch_size: Batch Size\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param source_vocab_size: Source vocabulary size\n",
    "    :param target_vocab_size: Target vocabulary size\n",
    "    :param enc_embedding_size: Decoder embedding size\n",
    "    :param dec_embedding_size: Encoder embedding size\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: Tuple of (Training Logits, Inference Logits)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     print(input_data)\n",
    "#     print(input_data.shape)\n",
    "    embed_input = tf.contrib.layers.embed_sequence(input_data, \n",
    "                                                   vocab_size = source_vocab_size, \n",
    "                                                   embed_dim = enc_embedding_size)\n",
    "#     , reuse = True)\n",
    "    '''\n",
    "                                                   vocab_size=None, \n",
    "                                                   embed_dim=None,\n",
    "                                                   unique=False,\n",
    "                                                   initializer=None,\n",
    "                                                   regularizer=None,\n",
    "                                                   trainable=True,\n",
    "                                                   scope=None)\n",
    "    '''\n",
    "\n",
    "    encoder_state = encoding_layer(embed_input,  rnn_size, num_layers, keep_prob)\n",
    "    print('22')\n",
    "    target_processed = process_decoding_input(target_data, target_vocab_to_int, batch_size)\n",
    "    print('33')\n",
    "#     target_embedded = tf.contrib.layers.embed_sequence(target_processed, \n",
    "#                                                     vocab_size = target_vocab_size, \n",
    "#                                                     embed_dim = dec_embedding_size)\n",
    "    print('44')\n",
    "    dec_embedding = tf.Variable(tf.random_uniform( (target_vocab_size, dec_embedding_size)))\n",
    "    print('45')\n",
    "    dec_embedded = tf.nn.embedding_lookup(dec_embedding, target_processed)\n",
    "    print('46')\n",
    "    train_logits, infer_logits = decoding_layer(dec_embedded, dec_embedding, encoder_state, \n",
    "                   target_vocab_size, sequence_length, rnn_size, num_layers, \n",
    "                   target_vocab_to_int, keep_prob)\n",
    "    print('55')\n",
    "\n",
    "    return train_logits, infer_logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_seq2seq_model(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络\n",
    "\n",
    "### 超参数\n",
    "\n",
    "调试以下参数：\n",
    "\n",
    "- 将 `epochs` 设为 epoch 次数。\n",
    "- 将 `batch_size` 设为批次大小。\n",
    "- 将 `rnn_size` 设为 RNN 的大小。\n",
    "- 将 `num_layers` 设为层级数量。\n",
    "- 将 `encoding_embedding_size` 设为编码器嵌入大小。\n",
    "- 将 `decoding_embedding_size` 设为解码器嵌入大小\n",
    "- 将 `learning_rate` 设为训练速率。\n",
    "- 将 `keep_probability` 设为丢弃保留率（Dropout keep probability）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 2\n",
    "# Batch Size\n",
    "batch_size = 8\n",
    "# RNN Size\n",
    "rnn_size = 128\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 64\n",
    "decoding_embedding_size = 64\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建图表\n",
    "\n",
    "使用你实现的神经网络构建图表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "(?, ?)\n",
      "Tensor(\"Fill:0\", shape=(8, 1), dtype=int32)\n",
      "Tensor(\"concat:0\", shape=(8, ?), dtype=int32)\n",
      "33\n",
      "44\n",
      "45\n",
      "46\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()\n",
    "max_source_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob = model_inputs()\n",
    "    sequence_length = tf.placeholder_with_default(max_source_sentence_length, None, name='sequence_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(\n",
    "        tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(source_vocab_to_int), len(target_vocab_to_int),\n",
    "        encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, target_vocab_to_int)\n",
    "\n",
    "    tf.identity(inference_logits, 'logits')\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            train_logits,\n",
    "            targets,\n",
    "            tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "\n",
    "利用预处理的数据训练神经网络。如果很难获得低损失值，请访问我们的论坛，看看其他人是否遇到了相同的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/17232 - Train Accuracy:  0.222, Validation Accuracy:  0.118, Loss:  5.879\n",
      "saved~~\n",
      "Epoch   0 Batch  100/17232 - Train Accuracy:  0.201, Validation Accuracy:  0.097, Loss:  3.072\n",
      "Epoch   0 Batch  200/17232 - Train Accuracy:  0.153, Validation Accuracy:  0.140, Loss:  2.196\n",
      "Epoch   0 Batch  300/17232 - Train Accuracy:  0.316, Validation Accuracy:  0.309, Loss:  2.167\n",
      "Epoch   0 Batch  400/17232 - Train Accuracy:  0.434, Validation Accuracy:  0.301, Loss:  2.169\n",
      "Epoch   0 Batch  500/17232 - Train Accuracy:  0.544, Validation Accuracy:  0.360, Loss:  1.919\n",
      "Epoch   0 Batch  600/17232 - Train Accuracy:  0.404, Validation Accuracy:  0.309, Loss:  2.151\n",
      "Epoch   0 Batch  700/17232 - Train Accuracy:  0.368, Validation Accuracy:  0.324, Loss:  1.897\n",
      "Epoch   0 Batch  800/17232 - Train Accuracy:  0.526, Validation Accuracy:  0.390, Loss:  2.103\n",
      "Epoch   0 Batch  900/17232 - Train Accuracy:  0.389, Validation Accuracy:  0.390, Loss:  1.814\n",
      "Epoch   0 Batch 1000/17232 - Train Accuracy:  0.487, Validation Accuracy:  0.340, Loss:  1.744\n",
      "saved~~\n",
      "Epoch   0 Batch 1100/17232 - Train Accuracy:  0.440, Validation Accuracy:  0.354, Loss:  1.649\n",
      "Epoch   0 Batch 1200/17232 - Train Accuracy:  0.520, Validation Accuracy:  0.354, Loss:  1.745\n",
      "Epoch   0 Batch 1300/17232 - Train Accuracy:  0.637, Validation Accuracy:  0.368, Loss:  1.534\n",
      "Epoch   0 Batch 1400/17232 - Train Accuracy:  0.729, Validation Accuracy:  0.434, Loss:  1.911\n",
      "Epoch   0 Batch 1500/17232 - Train Accuracy:  0.500, Validation Accuracy:  0.417, Loss:  2.014\n",
      "Epoch   0 Batch 1600/17232 - Train Accuracy:  0.701, Validation Accuracy:  0.472, Loss:  1.831\n",
      "Epoch   0 Batch 1700/17232 - Train Accuracy:  0.480, Validation Accuracy:  0.389, Loss:  1.850\n",
      "Epoch   0 Batch 1800/17232 - Train Accuracy:  0.590, Validation Accuracy:  0.403, Loss:  1.654\n",
      "Epoch   0 Batch 1900/17232 - Train Accuracy:  0.441, Validation Accuracy:  0.431, Loss:  1.801\n",
      "Epoch   0 Batch 2000/17232 - Train Accuracy:  0.546, Validation Accuracy:  0.493, Loss:  1.686\n",
      "saved~~\n",
      "Epoch   0 Batch 2100/17232 - Train Accuracy:  0.325, Validation Accuracy:  0.410, Loss:  1.468\n",
      "Epoch   0 Batch 2200/17232 - Train Accuracy:  0.493, Validation Accuracy:  0.354, Loss:  1.630\n",
      "Epoch   0 Batch 2300/17232 - Train Accuracy:  0.556, Validation Accuracy:  0.438, Loss:  1.721\n",
      "Epoch   0 Batch 2400/17232 - Train Accuracy:  0.539, Validation Accuracy:  0.486, Loss:  1.835\n",
      "Epoch   0 Batch 2500/17232 - Train Accuracy:  0.590, Validation Accuracy:  0.354, Loss:  1.706\n",
      "Epoch   0 Batch 2600/17232 - Train Accuracy:  0.375, Validation Accuracy:  0.417, Loss:  1.689\n",
      "Epoch   0 Batch 2700/17232 - Train Accuracy:  0.569, Validation Accuracy:  0.417, Loss:  1.617\n",
      "Epoch   0 Batch 2800/17232 - Train Accuracy:  0.604, Validation Accuracy:  0.382, Loss:  1.755\n",
      "Epoch   0 Batch 2900/17232 - Train Accuracy:  0.539, Validation Accuracy:  0.451, Loss:  1.818\n",
      "Epoch   0 Batch 3000/17232 - Train Accuracy:  0.424, Validation Accuracy:  0.549, Loss:  1.726\n",
      "saved~~\n",
      "Epoch   0 Batch 3100/17232 - Train Accuracy:  0.458, Validation Accuracy:  0.382, Loss:  1.956\n",
      "Epoch   0 Batch 3200/17232 - Train Accuracy:  0.421, Validation Accuracy:  0.535, Loss:  1.668\n",
      "Epoch   0 Batch 3300/17232 - Train Accuracy:  0.631, Validation Accuracy:  0.389, Loss:  1.729\n",
      "Epoch   0 Batch 3400/17232 - Train Accuracy:  0.604, Validation Accuracy:  0.458, Loss:  1.992\n",
      "Epoch   0 Batch 3500/17232 - Train Accuracy:  0.757, Validation Accuracy:  0.507, Loss:  1.706\n",
      "Epoch   0 Batch 3600/17232 - Train Accuracy:  0.611, Validation Accuracy:  0.507, Loss:  1.692\n",
      "Epoch   0 Batch 3700/17232 - Train Accuracy:  0.764, Validation Accuracy:  0.382, Loss:  1.561\n",
      "Epoch   0 Batch 3800/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.424, Loss:  1.923\n",
      "Epoch   0 Batch 3900/17232 - Train Accuracy:  0.556, Validation Accuracy:  0.403, Loss:  1.474\n",
      "Epoch   0 Batch 4000/17232 - Train Accuracy:  0.770, Validation Accuracy:  0.611, Loss:  1.659\n",
      "saved~~\n",
      "Epoch   0 Batch 4100/17232 - Train Accuracy:  0.487, Validation Accuracy:  0.493, Loss:  1.603\n",
      "Epoch   0 Batch 4200/17232 - Train Accuracy:  0.592, Validation Accuracy:  0.410, Loss:  1.688\n",
      "Epoch   0 Batch 4300/17232 - Train Accuracy:  0.625, Validation Accuracy:  0.382, Loss:  2.006\n",
      "Epoch   0 Batch 4400/17232 - Train Accuracy:  0.618, Validation Accuracy:  0.479, Loss:  1.404\n",
      "Epoch   0 Batch 4500/17232 - Train Accuracy:  0.599, Validation Accuracy:  0.493, Loss:  1.632\n",
      "Epoch   0 Batch 4600/17232 - Train Accuracy:  0.569, Validation Accuracy:  0.493, Loss:  1.227\n",
      "Epoch   0 Batch 4700/17232 - Train Accuracy:  0.525, Validation Accuracy:  0.479, Loss:  1.334\n",
      "Epoch   0 Batch 4800/17232 - Train Accuracy:  0.639, Validation Accuracy:  0.424, Loss:  1.586\n",
      "Epoch   0 Batch 4900/17232 - Train Accuracy:  0.579, Validation Accuracy:  0.396, Loss:  1.631\n",
      "Epoch   0 Batch 5000/17232 - Train Accuracy:  0.605, Validation Accuracy:  0.486, Loss:  1.602\n",
      "saved~~\n",
      "Epoch   0 Batch 5100/17232 - Train Accuracy:  0.743, Validation Accuracy:  0.458, Loss:  1.563\n",
      "Epoch   0 Batch 5200/17232 - Train Accuracy:  0.688, Validation Accuracy:  0.551, Loss:  1.577\n",
      "Epoch   0 Batch 5300/17232 - Train Accuracy:  0.783, Validation Accuracy:  0.486, Loss:  1.535\n",
      "Epoch   0 Batch 5400/17232 - Train Accuracy:  0.797, Validation Accuracy:  0.472, Loss:  1.760\n",
      "Epoch   0 Batch 5500/17232 - Train Accuracy:  0.644, Validation Accuracy:  0.493, Loss:  1.320\n",
      "Epoch   0 Batch 5600/17232 - Train Accuracy:  0.681, Validation Accuracy:  0.417, Loss:  1.766\n",
      "Epoch   0 Batch 5700/17232 - Train Accuracy:  0.653, Validation Accuracy:  0.312, Loss:  1.556\n",
      "Epoch   0 Batch 5800/17232 - Train Accuracy:  0.588, Validation Accuracy:  0.493, Loss:  1.835\n",
      "Epoch   0 Batch 5900/17232 - Train Accuracy:  0.785, Validation Accuracy:  0.583, Loss:  1.428\n",
      "Epoch   0 Batch 6000/17232 - Train Accuracy:  0.588, Validation Accuracy:  0.472, Loss:  1.710\n",
      "saved~~\n",
      "Epoch   0 Batch 6100/17232 - Train Accuracy:  0.691, Validation Accuracy:  0.544, Loss:  1.365\n",
      "Epoch   0 Batch 6200/17232 - Train Accuracy:  0.882, Validation Accuracy:  0.566, Loss:  1.430\n",
      "Epoch   0 Batch 6300/17232 - Train Accuracy:  0.676, Validation Accuracy:  0.611, Loss:  1.394\n",
      "Epoch   0 Batch 6400/17232 - Train Accuracy:  0.414, Validation Accuracy:  0.535, Loss:  1.554\n",
      "Epoch   0 Batch 6500/17232 - Train Accuracy:  0.674, Validation Accuracy:  0.632, Loss:  1.645\n",
      "Epoch   0 Batch 6600/17232 - Train Accuracy:  0.572, Validation Accuracy:  0.493, Loss:  1.822\n",
      "Epoch   0 Batch 6700/17232 - Train Accuracy:  0.478, Validation Accuracy:  0.569, Loss:  1.647\n",
      "Epoch   0 Batch 6800/17232 - Train Accuracy:  0.556, Validation Accuracy:  0.544, Loss:  1.486\n",
      "Epoch   0 Batch 6900/17232 - Train Accuracy:  0.604, Validation Accuracy:  0.603, Loss:  1.542\n",
      "Epoch   0 Batch 7000/17232 - Train Accuracy:  0.579, Validation Accuracy:  0.647, Loss:  1.714\n",
      "saved~~\n",
      "Epoch   0 Batch 7100/17232 - Train Accuracy:  0.592, Validation Accuracy:  0.660, Loss:  1.568\n",
      "Epoch   0 Batch 7200/17232 - Train Accuracy:  0.638, Validation Accuracy:  0.485, Loss:  1.470\n",
      "Epoch   0 Batch 7300/17232 - Train Accuracy:  0.684, Validation Accuracy:  0.537, Loss:  1.819\n",
      "Epoch   0 Batch 7400/17232 - Train Accuracy:  0.618, Validation Accuracy:  0.743, Loss:  1.562\n",
      "Epoch   0 Batch 7500/17232 - Train Accuracy:  0.725, Validation Accuracy:  0.559, Loss:  1.651\n",
      "Epoch   0 Batch 7600/17232 - Train Accuracy:  0.632, Validation Accuracy:  0.574, Loss:  1.373\n",
      "Epoch   0 Batch 7700/17232 - Train Accuracy:  0.743, Validation Accuracy:  0.660, Loss:  1.496\n",
      "Epoch   0 Batch 7800/17232 - Train Accuracy:  0.599, Validation Accuracy:  0.662, Loss:  1.661\n",
      "Epoch   0 Batch 7900/17232 - Train Accuracy:  0.653, Validation Accuracy:  0.647, Loss:  1.446\n",
      "Epoch   0 Batch 8000/17232 - Train Accuracy:  0.701, Validation Accuracy:  0.632, Loss:  1.595\n",
      "saved~~\n",
      "Epoch   0 Batch 8100/17232 - Train Accuracy:  0.875, Validation Accuracy:  0.757, Loss:  1.461\n",
      "Epoch   0 Batch 8200/17232 - Train Accuracy:  0.597, Validation Accuracy:  0.787, Loss:  1.580\n",
      "Epoch   0 Batch 8300/17232 - Train Accuracy:  0.729, Validation Accuracy:  0.706, Loss:  1.737\n",
      "Epoch   0 Batch 8400/17232 - Train Accuracy:  0.783, Validation Accuracy:  0.701, Loss:  1.423\n",
      "Epoch   0 Batch 8500/17232 - Train Accuracy:  0.764, Validation Accuracy:  0.757, Loss:  1.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 8600/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.713, Loss:  1.312\n",
      "Epoch   0 Batch 8700/17232 - Train Accuracy:  0.559, Validation Accuracy:  0.653, Loss:  1.637\n",
      "Epoch   0 Batch 8800/17232 - Train Accuracy:  0.724, Validation Accuracy:  0.611, Loss:  1.298\n",
      "Epoch   0 Batch 8900/17232 - Train Accuracy:  0.526, Validation Accuracy:  0.699, Loss:  1.521\n",
      "Epoch   0 Batch 9000/17232 - Train Accuracy:  0.708, Validation Accuracy:  0.801, Loss:  1.659\n",
      "saved~~\n",
      "Epoch   0 Batch 9100/17232 - Train Accuracy:  0.846, Validation Accuracy:  0.646, Loss:  1.536\n",
      "Epoch   0 Batch 9200/17232 - Train Accuracy:  0.724, Validation Accuracy:  0.674, Loss:  1.442\n",
      "Epoch   0 Batch 9300/17232 - Train Accuracy:  0.785, Validation Accuracy:  0.728, Loss:  1.753\n",
      "Epoch   0 Batch 9400/17232 - Train Accuracy:  0.736, Validation Accuracy:  0.713, Loss:  1.451\n",
      "Epoch   0 Batch 9500/17232 - Train Accuracy:  0.708, Validation Accuracy:  0.729, Loss:  1.318\n",
      "Epoch   0 Batch 9600/17232 - Train Accuracy:  0.678, Validation Accuracy:  0.743, Loss:  1.412\n",
      "Epoch   0 Batch 9700/17232 - Train Accuracy:  0.833, Validation Accuracy:  0.701, Loss:  1.677\n",
      "Epoch   0 Batch 9800/17232 - Train Accuracy:  0.604, Validation Accuracy:  0.736, Loss:  1.730\n",
      "Epoch   0 Batch 9900/17232 - Train Accuracy:  0.757, Validation Accuracy:  0.735, Loss:  1.625\n",
      "Epoch   0 Batch 10000/17232 - Train Accuracy:  0.846, Validation Accuracy:  0.722, Loss:  1.659\n",
      "saved~~\n",
      "Epoch   0 Batch 10100/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.816, Loss:  1.413\n",
      "Epoch   0 Batch 10200/17232 - Train Accuracy:  0.708, Validation Accuracy:  0.660, Loss:  1.381\n",
      "Epoch   0 Batch 10300/17232 - Train Accuracy:  0.785, Validation Accuracy:  0.676, Loss:  1.583\n",
      "Epoch   0 Batch 10400/17232 - Train Accuracy:  0.520, Validation Accuracy:  0.706, Loss:  1.717\n",
      "Epoch   0 Batch 10500/17232 - Train Accuracy:  0.507, Validation Accuracy:  0.569, Loss:  1.440\n",
      "Epoch   0 Batch 10600/17232 - Train Accuracy:  0.667, Validation Accuracy:  0.728, Loss:  1.643\n",
      "Epoch   0 Batch 10700/17232 - Train Accuracy:  0.803, Validation Accuracy:  0.521, Loss:  1.588\n",
      "Epoch   0 Batch 10800/17232 - Train Accuracy:  0.816, Validation Accuracy:  0.676, Loss:  1.408\n",
      "Epoch   0 Batch 10900/17232 - Train Accuracy:  0.775, Validation Accuracy:  0.639, Loss:  1.326\n",
      "Epoch   0 Batch 11000/17232 - Train Accuracy:  0.562, Validation Accuracy:  0.688, Loss:  1.734\n",
      "saved~~\n",
      "Epoch   0 Batch 11100/17232 - Train Accuracy:  0.662, Validation Accuracy:  0.721, Loss:  1.682\n",
      "Epoch   0 Batch 11200/17232 - Train Accuracy:  0.660, Validation Accuracy:  0.794, Loss:  1.367\n",
      "Epoch   0 Batch 11300/17232 - Train Accuracy:  0.664, Validation Accuracy:  0.713, Loss:  1.674\n",
      "Epoch   0 Batch 11400/17232 - Train Accuracy:  0.844, Validation Accuracy:  0.722, Loss:  1.706\n",
      "Epoch   0 Batch 11500/17232 - Train Accuracy:  0.838, Validation Accuracy:  0.765, Loss:  1.392\n",
      "Epoch   0 Batch 11600/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.676, Loss:  1.562\n",
      "Epoch   0 Batch 11700/17232 - Train Accuracy:  0.500, Validation Accuracy:  0.669, Loss:  1.395\n",
      "Epoch   0 Batch 11800/17232 - Train Accuracy:  0.638, Validation Accuracy:  0.729, Loss:  1.565\n",
      "Epoch   0 Batch 11900/17232 - Train Accuracy:  0.688, Validation Accuracy:  0.816, Loss:  1.317\n",
      "Epoch   0 Batch 12000/17232 - Train Accuracy:  0.757, Validation Accuracy:  0.779, Loss:  1.392\n",
      "saved~~\n",
      "Epoch   0 Batch 12100/17232 - Train Accuracy:  0.667, Validation Accuracy:  0.691, Loss:  1.650\n",
      "Epoch   0 Batch 12200/17232 - Train Accuracy:  0.500, Validation Accuracy:  0.824, Loss:  1.654\n",
      "Epoch   0 Batch 12300/17232 - Train Accuracy:  0.765, Validation Accuracy:  0.757, Loss:  1.469\n",
      "Epoch   0 Batch 12400/17232 - Train Accuracy:  0.763, Validation Accuracy:  0.838, Loss:  1.673\n",
      "Epoch   0 Batch 12500/17232 - Train Accuracy:  0.701, Validation Accuracy:  0.728, Loss:  1.693\n",
      "Epoch   0 Batch 12600/17232 - Train Accuracy:  0.724, Validation Accuracy:  0.728, Loss:  1.317\n",
      "Epoch   0 Batch 12700/17232 - Train Accuracy:  0.542, Validation Accuracy:  0.583, Loss:  1.388\n",
      "Epoch   0 Batch 12800/17232 - Train Accuracy:  0.553, Validation Accuracy:  0.632, Loss:  1.700\n",
      "Epoch   0 Batch 12900/17232 - Train Accuracy:  0.763, Validation Accuracy:  0.653, Loss:  1.372\n",
      "Epoch   0 Batch 13000/17232 - Train Accuracy:  0.809, Validation Accuracy:  0.556, Loss:  1.381\n",
      "saved~~\n",
      "Epoch   0 Batch 13100/17232 - Train Accuracy:  0.625, Validation Accuracy:  0.653, Loss:  1.387\n",
      "Epoch   0 Batch 13200/17232 - Train Accuracy:  0.787, Validation Accuracy:  0.632, Loss:  1.496\n",
      "Epoch   0 Batch 13300/17232 - Train Accuracy:  0.678, Validation Accuracy:  0.625, Loss:  1.587\n",
      "Epoch   0 Batch 13400/17232 - Train Accuracy:  0.743, Validation Accuracy:  0.647, Loss:  1.271\n",
      "Epoch   0 Batch 13500/17232 - Train Accuracy:  0.606, Validation Accuracy:  0.743, Loss:  1.341\n",
      "Epoch   0 Batch 13600/17232 - Train Accuracy:  0.632, Validation Accuracy:  0.757, Loss:  1.554\n",
      "Epoch   0 Batch 13700/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.728, Loss:  1.349\n",
      "Epoch   0 Batch 13800/17232 - Train Accuracy:  0.722, Validation Accuracy:  0.728, Loss:  1.418\n",
      "Epoch   0 Batch 13900/17232 - Train Accuracy:  0.717, Validation Accuracy:  0.757, Loss:  1.469\n",
      "Epoch   0 Batch 14000/17232 - Train Accuracy:  0.763, Validation Accuracy:  0.699, Loss:  1.551\n",
      "saved~~\n",
      "Epoch   0 Batch 14100/17232 - Train Accuracy:  0.646, Validation Accuracy:  0.771, Loss:  1.799\n",
      "Epoch   0 Batch 14200/17232 - Train Accuracy:  0.594, Validation Accuracy:  0.708, Loss:  1.470\n",
      "Epoch   0 Batch 14300/17232 - Train Accuracy:  0.708, Validation Accuracy:  0.765, Loss:  1.441\n",
      "Epoch   0 Batch 14400/17232 - Train Accuracy:  0.722, Validation Accuracy:  0.681, Loss:  1.745\n",
      "Epoch   0 Batch 14500/17232 - Train Accuracy:  0.724, Validation Accuracy:  0.688, Loss:  1.347\n",
      "Epoch   0 Batch 14600/17232 - Train Accuracy:  0.664, Validation Accuracy:  0.765, Loss:  1.273\n",
      "Epoch   0 Batch 14700/17232 - Train Accuracy:  0.729, Validation Accuracy:  0.667, Loss:  1.652\n",
      "Epoch   0 Batch 14800/17232 - Train Accuracy:  0.787, Validation Accuracy:  0.583, Loss:  1.687\n",
      "Epoch   0 Batch 14900/17232 - Train Accuracy:  0.669, Validation Accuracy:  0.728, Loss:  1.561\n",
      "Epoch   0 Batch 15000/17232 - Train Accuracy:  0.722, Validation Accuracy:  0.757, Loss:  1.183\n",
      "saved~~\n",
      "Epoch   0 Batch 15100/17232 - Train Accuracy:  0.882, Validation Accuracy:  0.735, Loss:  1.450\n",
      "Epoch   0 Batch 15200/17232 - Train Accuracy:  0.625, Validation Accuracy:  0.669, Loss:  1.335\n",
      "Epoch   0 Batch 15300/17232 - Train Accuracy:  0.618, Validation Accuracy:  0.590, Loss:  1.361\n",
      "Epoch   0 Batch 15400/17232 - Train Accuracy:  0.611, Validation Accuracy:  0.681, Loss:  1.546\n",
      "Epoch   0 Batch 15500/17232 - Train Accuracy:  0.743, Validation Accuracy:  0.688, Loss:  1.234\n",
      "Epoch   0 Batch 15600/17232 - Train Accuracy:  0.763, Validation Accuracy:  0.694, Loss:  1.304\n",
      "Epoch   0 Batch 15700/17232 - Train Accuracy:  0.662, Validation Accuracy:  0.625, Loss:  1.270\n",
      "Epoch   0 Batch 15800/17232 - Train Accuracy:  0.861, Validation Accuracy:  0.787, Loss:  1.395\n",
      "Epoch   0 Batch 15900/17232 - Train Accuracy:  0.697, Validation Accuracy:  0.706, Loss:  1.418\n",
      "Epoch   0 Batch 16000/17232 - Train Accuracy:  0.708, Validation Accuracy:  0.660, Loss:  1.391\n",
      "saved~~\n",
      "Epoch   0 Batch 16100/17232 - Train Accuracy:  0.639, Validation Accuracy:  0.694, Loss:  1.499\n",
      "Epoch   0 Batch 16200/17232 - Train Accuracy:  0.729, Validation Accuracy:  0.604, Loss:  1.198\n",
      "Epoch   0 Batch 16300/17232 - Train Accuracy:  0.743, Validation Accuracy:  0.583, Loss:  1.528\n",
      "Epoch   0 Batch 16400/17232 - Train Accuracy:  0.706, Validation Accuracy:  0.618, Loss:  1.194\n",
      "Epoch   0 Batch 16500/17232 - Train Accuracy:  0.729, Validation Accuracy:  0.653, Loss:  1.665\n",
      "Epoch   0 Batch 16600/17232 - Train Accuracy:  0.625, Validation Accuracy:  0.694, Loss:  1.360\n",
      "Epoch   0 Batch 16700/17232 - Train Accuracy:  0.667, Validation Accuracy:  0.735, Loss:  1.307\n",
      "Epoch   0 Batch 16800/17232 - Train Accuracy:  0.715, Validation Accuracy:  0.706, Loss:  1.569\n",
      "Epoch   0 Batch 16900/17232 - Train Accuracy:  0.656, Validation Accuracy:  0.676, Loss:  1.587\n",
      "Epoch   0 Batch 17000/17232 - Train Accuracy:  0.789, Validation Accuracy:  0.590, Loss:  1.632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved~~\n",
      "Epoch   0 Batch 17100/17232 - Train Accuracy:  0.592, Validation Accuracy:  0.597, Loss:  1.387\n",
      "Epoch   0 Batch 17200/17232 - Train Accuracy:  0.771, Validation Accuracy:  0.646, Loss:  1.193\n",
      "Epoch   1 Batch    0/17232 - Train Accuracy:  0.507, Validation Accuracy:  0.853, Loss:  1.578\n",
      "saved~~\n",
      "Epoch   1 Batch  100/17232 - Train Accuracy:  0.896, Validation Accuracy:  0.667, Loss:  1.473\n",
      "Epoch   1 Batch  200/17232 - Train Accuracy:  0.493, Validation Accuracy:  0.715, Loss:  1.588\n",
      "Epoch   1 Batch  300/17232 - Train Accuracy:  0.562, Validation Accuracy:  0.757, Loss:  1.673\n",
      "Epoch   1 Batch  400/17232 - Train Accuracy:  0.717, Validation Accuracy:  0.662, Loss:  1.192\n",
      "Epoch   1 Batch  500/17232 - Train Accuracy:  0.844, Validation Accuracy:  0.667, Loss:  1.360\n",
      "Epoch   1 Batch  600/17232 - Train Accuracy:  0.772, Validation Accuracy:  0.736, Loss:  1.446\n",
      "Epoch   1 Batch  700/17232 - Train Accuracy:  0.914, Validation Accuracy:  0.667, Loss:  1.799\n",
      "Epoch   1 Batch  800/17232 - Train Accuracy:  0.836, Validation Accuracy:  0.838, Loss:  1.414\n",
      "Epoch   1 Batch  900/17232 - Train Accuracy:  0.743, Validation Accuracy:  0.765, Loss:  1.401\n",
      "Epoch   1 Batch 1000/17232 - Train Accuracy:  0.606, Validation Accuracy:  0.831, Loss:  1.573\n",
      "saved~~\n",
      "Epoch   1 Batch 1100/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.713, Loss:  1.453\n",
      "Epoch   1 Batch 1200/17232 - Train Accuracy:  0.711, Validation Accuracy:  0.566, Loss:  1.570\n",
      "Epoch   1 Batch 1300/17232 - Train Accuracy:  0.794, Validation Accuracy:  0.681, Loss:  1.527\n",
      "Epoch   1 Batch 1400/17232 - Train Accuracy:  0.924, Validation Accuracy:  0.831, Loss:  1.139\n",
      "Epoch   1 Batch 1500/17232 - Train Accuracy:  0.681, Validation Accuracy:  0.750, Loss:  1.293\n",
      "Epoch   1 Batch 1600/17232 - Train Accuracy:  0.674, Validation Accuracy:  0.632, Loss:  1.391\n",
      "Epoch   1 Batch 1700/17232 - Train Accuracy:  0.599, Validation Accuracy:  0.750, Loss:  1.578\n",
      "Epoch   1 Batch 1800/17232 - Train Accuracy:  0.792, Validation Accuracy:  0.715, Loss:  1.247\n",
      "Epoch   1 Batch 1900/17232 - Train Accuracy:  0.467, Validation Accuracy:  0.562, Loss:  1.439\n",
      "Epoch   1 Batch 2000/17232 - Train Accuracy:  0.730, Validation Accuracy:  0.596, Loss:  1.502\n",
      "saved~~\n",
      "Epoch   1 Batch 2100/17232 - Train Accuracy:  0.525, Validation Accuracy:  0.535, Loss:  1.271\n",
      "Epoch   1 Batch 2200/17232 - Train Accuracy:  0.724, Validation Accuracy:  0.728, Loss:  1.752\n",
      "Epoch   1 Batch 2300/17232 - Train Accuracy:  0.662, Validation Accuracy:  0.728, Loss:  1.232\n",
      "Epoch   1 Batch 2400/17232 - Train Accuracy:  0.651, Validation Accuracy:  0.750, Loss:  1.620\n",
      "Epoch   1 Batch 2500/17232 - Train Accuracy:  0.847, Validation Accuracy:  0.728, Loss:  1.553\n",
      "Epoch   1 Batch 2600/17232 - Train Accuracy:  0.688, Validation Accuracy:  0.708, Loss:  1.530\n",
      "Epoch   1 Batch 2700/17232 - Train Accuracy:  0.854, Validation Accuracy:  0.735, Loss:  1.508\n",
      "Epoch   1 Batch 2800/17232 - Train Accuracy:  0.596, Validation Accuracy:  0.667, Loss:  1.347\n",
      "Epoch   1 Batch 2900/17232 - Train Accuracy:  0.632, Validation Accuracy:  0.676, Loss:  1.698\n",
      "Epoch   1 Batch 3000/17232 - Train Accuracy:  0.632, Validation Accuracy:  0.772, Loss:  1.317\n",
      "saved~~\n",
      "Epoch   1 Batch 3100/17232 - Train Accuracy:  0.715, Validation Accuracy:  0.721, Loss:  1.809\n",
      "Epoch   1 Batch 3200/17232 - Train Accuracy:  0.730, Validation Accuracy:  0.816, Loss:  1.404\n",
      "Epoch   1 Batch 3300/17232 - Train Accuracy:  0.806, Validation Accuracy:  0.838, Loss:  1.296\n",
      "Epoch   1 Batch 3400/17232 - Train Accuracy:  0.764, Validation Accuracy:  0.715, Loss:  1.662\n",
      "Epoch   1 Batch 3500/17232 - Train Accuracy:  0.729, Validation Accuracy:  0.583, Loss:  1.716\n",
      "Epoch   1 Batch 3600/17232 - Train Accuracy:  0.743, Validation Accuracy:  0.701, Loss:  1.976\n",
      "Epoch   1 Batch 3700/17232 - Train Accuracy:  0.875, Validation Accuracy:  0.669, Loss:  1.064\n",
      "Epoch   1 Batch 3800/17232 - Train Accuracy:  0.778, Validation Accuracy:  0.688, Loss:  1.370\n",
      "Epoch   1 Batch 3900/17232 - Train Accuracy:  0.819, Validation Accuracy:  0.676, Loss:  1.423\n",
      "Epoch   1 Batch 4000/17232 - Train Accuracy:  0.724, Validation Accuracy:  0.588, Loss:  1.228\n",
      "saved~~\n",
      "Epoch   1 Batch 4100/17232 - Train Accuracy:  0.737, Validation Accuracy:  0.604, Loss:  1.344\n",
      "Epoch   1 Batch 4200/17232 - Train Accuracy:  0.757, Validation Accuracy:  0.765, Loss:  1.704\n",
      "Epoch   1 Batch 4300/17232 - Train Accuracy:  0.711, Validation Accuracy:  0.831, Loss:  1.778\n",
      "Epoch   1 Batch 4400/17232 - Train Accuracy:  0.794, Validation Accuracy:  0.681, Loss:  1.392\n",
      "Epoch   1 Batch 4500/17232 - Train Accuracy:  0.757, Validation Accuracy:  0.604, Loss:  1.381\n",
      "Epoch   1 Batch 4600/17232 - Train Accuracy:  0.861, Validation Accuracy:  0.694, Loss:  1.603\n",
      "Epoch   1 Batch 4700/17232 - Train Accuracy:  0.613, Validation Accuracy:  0.632, Loss:  1.521\n",
      "Epoch   1 Batch 4800/17232 - Train Accuracy:  0.875, Validation Accuracy:  0.676, Loss:  1.391\n",
      "Epoch   1 Batch 4900/17232 - Train Accuracy:  0.730, Validation Accuracy:  0.694, Loss:  1.322\n",
      "Epoch   1 Batch 5000/17232 - Train Accuracy:  0.625, Validation Accuracy:  0.611, Loss:  1.439\n",
      "saved~~\n",
      "Epoch   1 Batch 5100/17232 - Train Accuracy:  0.849, Validation Accuracy:  0.674, Loss:  1.349\n",
      "Epoch   1 Batch 5200/17232 - Train Accuracy:  0.833, Validation Accuracy:  0.715, Loss:  1.252\n",
      "Epoch   1 Batch 5300/17232 - Train Accuracy:  0.763, Validation Accuracy:  0.676, Loss:  1.469\n",
      "Epoch   1 Batch 5400/17232 - Train Accuracy:  0.812, Validation Accuracy:  0.521, Loss:  1.619\n",
      "Epoch   1 Batch 5500/17232 - Train Accuracy:  0.738, Validation Accuracy:  0.583, Loss:  1.417\n",
      "Epoch   1 Batch 5600/17232 - Train Accuracy:  0.708, Validation Accuracy:  0.618, Loss:  1.478\n",
      "Epoch   1 Batch 5700/17232 - Train Accuracy:  0.715, Validation Accuracy:  0.528, Loss:  1.551\n",
      "Epoch   1 Batch 5800/17232 - Train Accuracy:  0.660, Validation Accuracy:  0.486, Loss:  1.518\n",
      "Epoch   1 Batch 5900/17232 - Train Accuracy:  0.715, Validation Accuracy:  0.611, Loss:  1.589\n",
      "Epoch   1 Batch 6000/17232 - Train Accuracy:  0.700, Validation Accuracy:  0.562, Loss:  1.191\n",
      "saved~~\n",
      "Epoch   1 Batch 6100/17232 - Train Accuracy:  0.770, Validation Accuracy:  0.688, Loss:  1.602\n",
      "Epoch   1 Batch 6200/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.618, Loss:  1.275\n",
      "Epoch   1 Batch 6300/17232 - Train Accuracy:  0.779, Validation Accuracy:  0.684, Loss:  1.679\n",
      "Epoch   1 Batch 6400/17232 - Train Accuracy:  0.625, Validation Accuracy:  0.674, Loss:  1.469\n",
      "Epoch   1 Batch 6500/17232 - Train Accuracy:  0.819, Validation Accuracy:  0.694, Loss:  1.728\n",
      "Epoch   1 Batch 6600/17232 - Train Accuracy:  0.500, Validation Accuracy:  0.701, Loss:  1.443\n",
      "Epoch   1 Batch 6700/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.625, Loss:  1.763\n",
      "Epoch   1 Batch 6800/17232 - Train Accuracy:  0.819, Validation Accuracy:  0.669, Loss:  1.225\n",
      "Epoch   1 Batch 6900/17232 - Train Accuracy:  0.861, Validation Accuracy:  0.728, Loss:  1.550\n",
      "Epoch   1 Batch 7000/17232 - Train Accuracy:  0.605, Validation Accuracy:  0.750, Loss:  1.210\n",
      "saved~~\n",
      "Epoch   1 Batch 7100/17232 - Train Accuracy:  0.803, Validation Accuracy:  0.611, Loss:  1.282\n",
      "Epoch   1 Batch 7200/17232 - Train Accuracy:  0.671, Validation Accuracy:  0.688, Loss:  1.702\n",
      "Epoch   1 Batch 7300/17232 - Train Accuracy:  0.838, Validation Accuracy:  0.765, Loss:  1.474\n",
      "Epoch   1 Batch 7400/17232 - Train Accuracy:  0.833, Validation Accuracy:  0.691, Loss:  1.459\n",
      "Epoch   1 Batch 7500/17232 - Train Accuracy:  0.812, Validation Accuracy:  0.588, Loss:  1.425\n",
      "Epoch   1 Batch 7600/17232 - Train Accuracy:  0.792, Validation Accuracy:  0.772, Loss:  1.330\n",
      "Epoch   1 Batch 7700/17232 - Train Accuracy:  0.776, Validation Accuracy:  0.604, Loss:  1.440\n",
      "Epoch   1 Batch 7800/17232 - Train Accuracy:  0.638, Validation Accuracy:  0.604, Loss:  1.558\n",
      "Epoch   1 Batch 7900/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.618, Loss:  1.171\n",
      "Epoch   1 Batch 8000/17232 - Train Accuracy:  0.736, Validation Accuracy:  0.618, Loss:  1.455\n",
      "saved~~\n",
      "Epoch   1 Batch 8100/17232 - Train Accuracy:  0.779, Validation Accuracy:  0.787, Loss:  1.681\n",
      "Epoch   1 Batch 8200/17232 - Train Accuracy:  0.562, Validation Accuracy:  0.757, Loss:  1.364\n",
      "Epoch   1 Batch 8300/17232 - Train Accuracy:  0.771, Validation Accuracy:  0.824, Loss:  1.669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch 8400/17232 - Train Accuracy:  0.855, Validation Accuracy:  0.729, Loss:  1.230\n",
      "Epoch   1 Batch 8500/17232 - Train Accuracy:  0.736, Validation Accuracy:  0.660, Loss:  1.473\n",
      "Epoch   1 Batch 8600/17232 - Train Accuracy:  0.728, Validation Accuracy:  0.713, Loss:  1.130\n",
      "Epoch   1 Batch 8700/17232 - Train Accuracy:  0.632, Validation Accuracy:  0.684, Loss:  1.221\n",
      "Epoch   1 Batch 8800/17232 - Train Accuracy:  0.809, Validation Accuracy:  0.736, Loss:  1.208\n",
      "Epoch   1 Batch 8900/17232 - Train Accuracy:  0.559, Validation Accuracy:  0.674, Loss:  1.098\n",
      "Epoch   1 Batch 9000/17232 - Train Accuracy:  0.736, Validation Accuracy:  0.809, Loss:  1.473\n",
      "saved~~\n",
      "Epoch   1 Batch 9100/17232 - Train Accuracy:  0.799, Validation Accuracy:  0.765, Loss:  1.691\n",
      "Epoch   1 Batch 9200/17232 - Train Accuracy:  0.842, Validation Accuracy:  0.801, Loss:  1.228\n",
      "Epoch   1 Batch 9300/17232 - Train Accuracy:  0.722, Validation Accuracy:  0.618, Loss:  1.770\n",
      "Epoch   1 Batch 9400/17232 - Train Accuracy:  0.806, Validation Accuracy:  0.801, Loss:  1.497\n",
      "Epoch   1 Batch 9500/17232 - Train Accuracy:  0.764, Validation Accuracy:  0.757, Loss:  1.496\n",
      "Epoch   1 Batch 9600/17232 - Train Accuracy:  0.711, Validation Accuracy:  0.772, Loss:  1.388\n",
      "Epoch   1 Batch 9700/17232 - Train Accuracy:  0.875, Validation Accuracy:  0.779, Loss:  1.517\n",
      "Epoch   1 Batch 9800/17232 - Train Accuracy:  0.722, Validation Accuracy:  0.699, Loss:  1.736\n",
      "Epoch   1 Batch 9900/17232 - Train Accuracy:  0.735, Validation Accuracy:  0.662, Loss:  1.685\n",
      "Epoch   1 Batch 10000/17232 - Train Accuracy:  0.875, Validation Accuracy:  0.779, Loss:  1.310\n",
      "saved~~\n",
      "Epoch   1 Batch 10100/17232 - Train Accuracy:  0.819, Validation Accuracy:  0.743, Loss:  1.202\n",
      "Epoch   1 Batch 10200/17232 - Train Accuracy:  0.764, Validation Accuracy:  0.728, Loss:  1.104\n",
      "Epoch   1 Batch 10300/17232 - Train Accuracy:  0.743, Validation Accuracy:  0.691, Loss:  1.363\n",
      "Epoch   1 Batch 10400/17232 - Train Accuracy:  0.539, Validation Accuracy:  0.618, Loss:  1.639\n",
      "Epoch   1 Batch 10500/17232 - Train Accuracy:  0.562, Validation Accuracy:  0.750, Loss:  1.584\n",
      "Epoch   1 Batch 10600/17232 - Train Accuracy:  0.694, Validation Accuracy:  0.596, Loss:  1.502\n",
      "Epoch   1 Batch 10700/17232 - Train Accuracy:  0.776, Validation Accuracy:  0.765, Loss:  1.359\n",
      "Epoch   1 Batch 10800/17232 - Train Accuracy:  0.706, Validation Accuracy:  0.838, Loss:  1.694\n",
      "Epoch   1 Batch 10900/17232 - Train Accuracy:  0.800, Validation Accuracy:  0.757, Loss:  1.399\n",
      "Epoch   1 Batch 11000/17232 - Train Accuracy:  0.576, Validation Accuracy:  0.765, Loss:  1.554\n",
      "saved~~\n",
      "Epoch   1 Batch 11100/17232 - Train Accuracy:  0.890, Validation Accuracy:  0.816, Loss:  1.203\n",
      "Epoch   1 Batch 11200/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.846, Loss:  1.412\n",
      "Epoch   1 Batch 11300/17232 - Train Accuracy:  0.684, Validation Accuracy:  0.743, Loss:  1.307\n",
      "Epoch   1 Batch 11400/17232 - Train Accuracy:  0.819, Validation Accuracy:  0.694, Loss:  1.199\n",
      "Epoch   1 Batch 11500/17232 - Train Accuracy:  0.772, Validation Accuracy:  0.715, Loss:  1.384\n",
      "Epoch   1 Batch 11600/17232 - Train Accuracy:  0.806, Validation Accuracy:  0.853, Loss:  1.410\n",
      "Epoch   1 Batch 11700/17232 - Train Accuracy:  0.671, Validation Accuracy:  0.787, Loss:  1.296\n",
      "Epoch   1 Batch 11800/17232 - Train Accuracy:  0.658, Validation Accuracy:  0.750, Loss:  1.266\n",
      "Epoch   1 Batch 11900/17232 - Train Accuracy:  0.800, Validation Accuracy:  0.794, Loss:  1.432\n",
      "Epoch   1 Batch 12000/17232 - Train Accuracy:  0.736, Validation Accuracy:  0.787, Loss:  1.603\n",
      "saved~~\n",
      "Epoch   1 Batch 12100/17232 - Train Accuracy:  0.729, Validation Accuracy:  0.706, Loss:  1.327\n",
      "Epoch   1 Batch 12200/17232 - Train Accuracy:  0.605, Validation Accuracy:  0.684, Loss:  1.120\n",
      "Epoch   1 Batch 12300/17232 - Train Accuracy:  0.676, Validation Accuracy:  0.765, Loss:  1.511\n",
      "Epoch   1 Batch 12400/17232 - Train Accuracy:  0.717, Validation Accuracy:  0.750, Loss:  1.466\n",
      "Epoch   1 Batch 12500/17232 - Train Accuracy:  0.611, Validation Accuracy:  0.647, Loss:  1.437\n",
      "Epoch   1 Batch 12600/17232 - Train Accuracy:  0.658, Validation Accuracy:  0.706, Loss:  1.040\n",
      "Epoch   1 Batch 12700/17232 - Train Accuracy:  0.535, Validation Accuracy:  0.646, Loss:  1.203\n",
      "Epoch   1 Batch 12800/17232 - Train Accuracy:  0.684, Validation Accuracy:  0.676, Loss:  1.712\n",
      "Epoch   1 Batch 12900/17232 - Train Accuracy:  0.849, Validation Accuracy:  0.831, Loss:  1.318\n",
      "Epoch   1 Batch 13000/17232 - Train Accuracy:  0.842, Validation Accuracy:  0.801, Loss:  1.442\n",
      "saved~~\n",
      "Epoch   1 Batch 13100/17232 - Train Accuracy:  0.669, Validation Accuracy:  0.596, Loss:  1.344\n",
      "Epoch   1 Batch 13200/17232 - Train Accuracy:  0.713, Validation Accuracy:  0.706, Loss:  1.448\n",
      "Epoch   1 Batch 13300/17232 - Train Accuracy:  0.678, Validation Accuracy:  0.674, Loss:  1.572\n",
      "Epoch   1 Batch 13400/17232 - Train Accuracy:  0.803, Validation Accuracy:  0.750, Loss:  1.249\n",
      "Epoch   1 Batch 13500/17232 - Train Accuracy:  0.713, Validation Accuracy:  0.691, Loss:  1.738\n",
      "Epoch   1 Batch 13600/17232 - Train Accuracy:  0.771, Validation Accuracy:  0.779, Loss:  1.468\n",
      "Epoch   1 Batch 13700/17232 - Train Accuracy:  0.701, Validation Accuracy:  0.681, Loss:  1.317\n",
      "Epoch   1 Batch 13800/17232 - Train Accuracy:  0.653, Validation Accuracy:  0.691, Loss:  1.538\n",
      "Epoch   1 Batch 13900/17232 - Train Accuracy:  0.704, Validation Accuracy:  0.701, Loss:  1.291\n",
      "Epoch   1 Batch 14000/17232 - Train Accuracy:  0.763, Validation Accuracy:  0.721, Loss:  1.546\n",
      "saved~~\n",
      "Epoch   1 Batch 14100/17232 - Train Accuracy:  0.764, Validation Accuracy:  0.750, Loss:  1.888\n",
      "Epoch   1 Batch 14200/17232 - Train Accuracy:  0.669, Validation Accuracy:  0.691, Loss:  1.502\n",
      "Epoch   1 Batch 14300/17232 - Train Accuracy:  0.486, Validation Accuracy:  0.743, Loss:  1.456\n",
      "Epoch   1 Batch 14400/17232 - Train Accuracy:  0.826, Validation Accuracy:  0.794, Loss:  1.385\n",
      "Epoch   1 Batch 14500/17232 - Train Accuracy:  0.901, Validation Accuracy:  0.824, Loss:  1.655\n",
      "Epoch   1 Batch 14600/17232 - Train Accuracy:  0.704, Validation Accuracy:  0.706, Loss:  1.660\n",
      "Epoch   1 Batch 14700/17232 - Train Accuracy:  0.819, Validation Accuracy:  0.743, Loss:  1.772\n",
      "Epoch   1 Batch 14800/17232 - Train Accuracy:  0.816, Validation Accuracy:  0.610, Loss:  1.552\n",
      "Epoch   1 Batch 14900/17232 - Train Accuracy:  0.875, Validation Accuracy:  0.590, Loss:  1.591\n",
      "Epoch   1 Batch 15000/17232 - Train Accuracy:  0.819, Validation Accuracy:  0.583, Loss:  1.318\n",
      "saved~~\n",
      "Epoch   1 Batch 15100/17232 - Train Accuracy:  0.882, Validation Accuracy:  0.694, Loss:  1.378\n",
      "Epoch   1 Batch 15200/17232 - Train Accuracy:  0.732, Validation Accuracy:  0.583, Loss:  1.502\n",
      "Epoch   1 Batch 15300/17232 - Train Accuracy:  0.801, Validation Accuracy:  0.688, Loss:  1.505\n",
      "Epoch   1 Batch 15400/17232 - Train Accuracy:  0.625, Validation Accuracy:  0.757, Loss:  1.532\n",
      "Epoch   1 Batch 15500/17232 - Train Accuracy:  0.678, Validation Accuracy:  0.590, Loss:  1.572\n",
      "Epoch   1 Batch 15600/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.757, Loss:  1.583\n",
      "Epoch   1 Batch 15700/17232 - Train Accuracy:  0.800, Validation Accuracy:  0.846, Loss:  1.281\n",
      "Epoch   1 Batch 15800/17232 - Train Accuracy:  0.917, Validation Accuracy:  0.831, Loss:  1.236\n",
      "Epoch   1 Batch 15900/17232 - Train Accuracy:  0.724, Validation Accuracy:  0.757, Loss:  1.537\n",
      "Epoch   1 Batch 16000/17232 - Train Accuracy:  0.688, Validation Accuracy:  0.875, Loss:  1.208\n",
      "saved~~\n",
      "Epoch   1 Batch 16100/17232 - Train Accuracy:  0.660, Validation Accuracy:  0.750, Loss:  1.803\n",
      "Epoch   1 Batch 16200/17232 - Train Accuracy:  0.750, Validation Accuracy:  0.632, Loss:  1.838\n",
      "Epoch   1 Batch 16300/17232 - Train Accuracy:  0.743, Validation Accuracy:  0.691, Loss:  1.677\n",
      "Epoch   1 Batch 16400/17232 - Train Accuracy:  0.787, Validation Accuracy:  0.654, Loss:  1.276\n",
      "Epoch   1 Batch 16500/17232 - Train Accuracy:  0.799, Validation Accuracy:  0.688, Loss:  1.148\n",
      "Epoch   1 Batch 16600/17232 - Train Accuracy:  0.625, Validation Accuracy:  0.853, Loss:  1.568\n",
      "Epoch   1 Batch 16700/17232 - Train Accuracy:  0.771, Validation Accuracy:  0.728, Loss:  1.220\n",
      "Epoch   1 Batch 16800/17232 - Train Accuracy:  0.715, Validation Accuracy:  0.674, Loss:  1.276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch 16900/17232 - Train Accuracy:  0.581, Validation Accuracy:  0.618, Loss:  1.530\n",
      "Epoch   1 Batch 17000/17232 - Train Accuracy:  0.855, Validation Accuracy:  0.838, Loss:  1.341\n",
      "saved~~\n",
      "Epoch   1 Batch 17100/17232 - Train Accuracy:  0.546, Validation Accuracy:  0.597, Loss:  1.296\n",
      "Epoch   1 Batch 17200/17232 - Train Accuracy:  0.792, Validation Accuracy:  0.521, Loss:  1.407\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import time\n",
    "\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1]), (0,0)],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, np.argmax(logits, 2)))\n",
    "\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "\n",
    "valid_source = helper.pad_sentence_batch(source_int_text[:batch_size])\n",
    "valid_target = helper.pad_sentence_batch(target_int_text[:batch_size])\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     saver = tf.train.Saver()\n",
    "#     saver.save(sess, save_path)\n",
    "#     print('saved')\n",
    "    \n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch) in enumerate(\n",
    "                helper.batch_data(train_source, train_target, batch_size)):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 sequence_length: target_batch.shape[1],\n",
    "                 keep_prob: keep_probability})\n",
    "            \n",
    "            batch_train_logits = sess.run(\n",
    "                inference_logits,\n",
    "                {input_data: source_batch, keep_prob: 1.0})\n",
    "            batch_valid_logits = sess.run(\n",
    "                inference_logits,\n",
    "                {input_data: valid_source, keep_prob: 1.0})\n",
    "                \n",
    "            train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "            valid_acc = get_accuracy(np.array(valid_target), batch_valid_logits)\n",
    "            end_time = time.time()\n",
    "            if batch_i %100 ==0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.3f}, Validation Accuracy: {:>6.3f}, Loss: {:>6.3f}'\n",
    "                  .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "            if batch_i %1000 == 0:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, save_path)\n",
    "                print('saved~~')\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存参数\n",
    "\n",
    "保存 `batch_size` 和 `save_path` 参数以进行推论（for inference）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()\n",
    "load_path = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句子到序列\n",
    "\n",
    "要向模型提供要翻译的句子，你首先需要预处理该句子。实现函数 `sentence_to_seq()` 以预处理新的句子。\n",
    "\n",
    "- 将句子转换为小写形式\n",
    "- 使用 `vocab_to_int` 将单词转换为 id\n",
    " - 如果单词不在词汇表中，将其转换为`<UNK>` 单词 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    setence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    words_ints = []\n",
    "    for word in words:\n",
    "        if word in vocab_to_int:\n",
    "            words_ints.append(vocab_to_int[word])\n",
    "        else:\n",
    "            words_ints.append(vocab_to_int['<UNK>'])\n",
    "    return words_ints\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_sentence_to_seq(sentence_to_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 翻译\n",
    "\n",
    "将 `translate_sentence` 从英语翻译成法语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "  Word Ids:      [162, 213, 112, 187, 96, 114, 127]\n",
      "  English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [11, 139, 56, 71, 232, 25, 293, 241, 1]\n",
      "  French Words: ['il', 'a', 'vu', 'la', 'voiture', 'rouillée', 'noir', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = 'he saw a old yellow truck .'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence], keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in np.argmax(translate_logits, 1)]))\n",
    "print('  French Words: {}'.format([target_int_to_vocab[i] for i in np.argmax(translate_logits, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不完美的翻译\n",
    "\n",
    "你可能注意到了，某些句子的翻译质量比其他的要好。因为你使用的数据集只有 227 个英语单词，但实际生活中有数千个单词，只有使用这些单词的句子结果才会比较理想。对于此项目，不需要达到完美的翻译。但是，如果你想创建更好的翻译模型，则需要更好的数据。\n",
    "\n",
    "你可以使用 [WMT10 French-English corpus](http://www.statmt.org/wmt10/training-giga-fren.tar) 语料库训练模型。该数据集拥有更多的词汇，讨论的话题也更丰富。但是，训练时间要好多天的时间，所以确保你有 GPU 并且对于我们提供的数据集，你的神经网络性能很棒。提交此项目后，别忘了研究下 WMT10 语料库。\n",
    "\n",
    "\n",
    "## 提交项目\n",
    "\n",
    "提交项目时，确保先运行所有单元，然后再保存记事本。保存记事本文件为 “dlnd_language_translation.ipynb”，再通过菜单中的“文件” ->“下载为”将其另存为 HTML 格式。提交的项目文档中需包含“helper.py”和“problem_unittests.py”文件。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
